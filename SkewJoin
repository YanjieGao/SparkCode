import org.apache.spark.SparkContext
import  org.apache.spark.SparkContext._

/**
 * Created by Administrator on 2014/8/16.
 *倾斜连接算法，将倾斜key抽出新建RDD，分别与需要连接的表进行连接
 */
object SkewJoin {
  def main(args : Array[String]) {
    val skewedTable = left.execute()
    val spark = new SparkContext("local", "TopK",
      System.getenv("SPARK_HOME"), SparkContext.jarOfClass(this.getClass))
    //存在数据倾斜的数据表
    val skewTable = spark.parallelize(1 to n ).map(row => (row, row))
    //与skewTable进行连接的表
    val Table =  spark.parallelize(1 to n ).map(row => (row, row))
    //对数据倾斜的表进行采样，假设只有一个key倾斜最严重。获取倾斜最大的key
    val sample = skewTable.sample(false, 0.3, 9).groupByKey().collect()
    val maxrowKey = sample.map(rows => (rows._2.size, rows._1)).maxBy(rows => rows._1)._2)
   //将倾斜的表拆分为两个RDD，一个为只含有倾斜key的表，一个为不含有倾斜key的表。
   //分别与原表进行连接
    val maxKeySkewedTable = skewTable.filter(row => {
      buildSideKeyGenerator(row) == maxrowKey
    })
    val mainSkewedTable = skewTable.filter(row => {
      !(buildSideKeyGenerator(row) == maxrowKey)
    })
    //分别与原表进行连接
    val maxKeyJoinedRdd = maxKeySkewedTable.join(Table)
    val mainJoinedRdd = mainSkewedTable.join(Table)
    //将结果进行合并
    sc.union(maxKeyJoinedRdd, mainJoinedRdd)
  }
}
